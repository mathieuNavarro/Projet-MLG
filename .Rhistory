library(lme4) # lmer(): To fit mixed-model
library(lmerTest) # lmer (): To fit mixed-model and diplay p-values
library(nlme) # To fit mixed-model
library(lattice) # To plot mixed-model
library(nlme)
library(plotly)
library(ggplot2)
library(Metrics)
library(gridExtra)
library(dplyr)
library(MASS)
library(Matrix)
library(tidyverse)
library(xgboost)
library(caret)
library(readxl)
library(kableExtra)
library(knitr)
library(corrplot)
theme_set(theme_bw())
set.seed(2023)
train<-read.csv('train_maladie.csv', header = T, sep = ",",dec=".")
testX<-read.csv('test_X_maladie.csv', header = T, sep = ",",dec=".")
train%>% rmarkdown::paged_table()
train2<-train
# Suppression des colonnes qu'on ne veut pas normaliser
genre=train$genre
sujet=train$sujet
score=train$score
duree=train$duree
train=subset(train,select=-c(genre,sujet,score,duree))
# Normalisation des données
train=scale(train)
train=as.data.frame(train)
# Rajout des variables enlevées pour la normalisation
train$genre=genre
train$sujet=sujet
train$score=score
train$duree=duree
# Déclaration des variables sujet et genre comme des facteurs
train$sujet=as.factor(train$sujet)
train$genre=as.factor(train$genre)
# Entrainement de notre modèle final sur tout le jeu de données :
# Création de notre variable inflx :
train$genre=as.numeric(train$genre)
moment_inflexion=numeric(42)
# Aggrégation du jeu de données pour les durées similaires par sujet, pour éviter les problèmes calculatoires du taux de variation.
for (i in 1:42){
patient<-train %>% filter(sujet ==i)
patient$duree=round(patient$duree)
patient <- aggregate(patient[c("age","genre","score","FF","FF.Abs","FF.RAP","FF.PPQ5","FF.DDP","AV","AV.dB","AV.APQ3","AV.APQ5","AV.APQ11","AV.DDA","BTC1","BTC2","CDNL","EFS","VFNL")], by = list(patient$duree), FUN = mean)
names(patient)[names(patient) == "Group.1"] <- "duree"
coeff_directeur <- numeric(0)
# Calcul des taux de variations entre chaque observation pour tous les sujets
for (j in 1:(length(patient$duree)-1)) {
coeff_directeur <- c(coeff_directeur, (patient$score[j+1] - patient$score[j]) / (patient$duree[j+1] - patient$duree[j]))
}
# Calcul de l'écart entre les taux de variations en valeur absolue
variations=abs(diff(coeff_directeur))
#Recherche de l'écart maximal
indice_max_variation_patient <- which.max(variations)+1
#Stockage dans le vecteur moment_inflexion
moment_inflexion[i]=(patient$duree[indice_max_variation_patient]+patient$duree[indice_max_variation_patient+1])/2
}
train <- train %>%
group_by(sujet) %>%
mutate(inflexion = moment_inflexion[sujet]) %>%
ungroup()
# Création de la variable inflx :
train$inflx=(train$duree>train$inflexion)
# Retrait de la variable inflexion, inutile pour nous
train=subset(train,select=-inflexion)
# Transformation de la variable inflx en numérique
train$inflx=as.numeric(train$inflx)
testX2<-testX
# Suppression des colonnes qu'on ne veut pas normaliser
genre=testX$genre
sujet=testX$sujet
duree=testX$duree
testX=subset(testX,select=-c(genre,sujet,duree))
# Normalisation des données
testX=scale(testX)
testX=as.data.frame(testX)
# Rajout des variables enlevées pour la normalisation
testX$genre=genre
testX$sujet=sujet
testX$duree=duree
# Déclaration des variables sujet et genre comme des facteurs
testX$sujet=as.factor(testX$sujet)
testX$genre=as.factor(testX$genre)
View(testX)
# Ajout de la variable inflx
testX <- testX %>%
group_by(sujet) %>%
mutate(inflexion = moment_inflexion[sujet]) %>%
ungroup()
testX$inflx=(testX$duree>testX$inflexion)
testX=subset(testX,select=-inflexion)
testX$inflx=as.numeric(testX$inflx)
View(testX)
mod_Final=lmer(score ~ duree+FF+AV+BTC1+CDNL+EFS+VFNL +(duree+BTC1+FF+AV+EFS+VFNL|  sujet:inflx), data = train, REML = FALSE)
RMSE_mod_Final= rmse(train$score,predict(mod_Final))
resultats_mod_Final=data.frame(
sujet=train$sujet,
duree=train$duree,
score=train$score,
predict=predict(mod_Final,train)
)
resultats_mod_Final %>% filter(sujet %in% selected) %>%
filter(sujet %in% selected) %>%
ggplot() +
geom_point(aes(x = duree, y = score, color = "Valeur du score"), size = 1) +
geom_line(aes(x = duree, y = predict, color = "Prédiction")) +
facet_wrap(~ sujet, ncol = 4) +
scale_color_manual(values = c("Valeur du score" = "red", "Prédiction" = "black"),
labels = c("Prédiction","Valeur du score")) +
labs(title = "Prédiction du modèle XG Boost sur le jeu de données Validation", color = "Légende")
selected=c(3,7,11,17,21,31,32,41)
train %>% filter(sujet %in% selected) %>%
ggplot() + geom_point(aes(x = FF.Abs, y = score)) + facet_wrap(~ sujet, ncol=2)
resultats_mod_Final=data.frame(
sujet=train$sujet,
duree=train$duree,
score=train$score,
predict=predict(mod_Final,train)
)
resultats_mod_Final %>% filter(sujet %in% selected) %>%
filter(sujet %in% selected) %>%
ggplot() +
geom_point(aes(x = duree, y = score, color = "Valeur du score"), size = 1) +
geom_line(aes(x = duree, y = predict, color = "Prédiction")) +
facet_wrap(~ sujet, ncol = 4) +
scale_color_manual(values = c("Valeur du score" = "red", "Prédiction" = "black"),
labels = c("Prédiction","Valeur du score")) +
labs(title = "Prédiction du modèle XG Boost sur le jeu de données Validation", color = "Légende")
resultats_mod_Final=data.frame(
sujet=train$sujet,
duree=train$duree,
score=train$score,
predict=predict(mod_Final,train)
)
resultats_mod_Final %>% filter(sujet %in% selected) %>%
filter(sujet %in% selected) %>%
ggplot() +
geom_point(aes(x = duree, y = score, color = "Valeur du score"), size = 1) +
geom_line(aes(x = duree, y = predict, color = "Prédiction")) +
facet_wrap(~ sujet, ncol = 4) +
scale_color_manual(values = c("Valeur du score" = "red", "Prédiction" = "black"),
labels = c("Prédiction","Valeur du score")) +
labs(title = "Prédiction du mod_Final sur tout le jeu de données Train", color = "Légende")
hat_y<-as.data.frame(predict(mod_Final,newdata=testX))
write_csv(hat_y,'hat_y.csv')
y_train <- train2$score
X_train <- as.matrix(train2[, -which(names(data_train) == "score")])
y_train <- train2$score
X_train <- as.matrix(train2[, -which(names(train2) == "score")])
X_test <- as.matrix(testX2)
# Entraîner le modèle XGBoost avec une grille de recherche des hyperparamètres
xgb_grid <- expand.grid(
nrounds = 150, # Nombre d'itérations
max_depth = c(10,11,12), # Profondeur maximale de l'arbre
eta = c(0.08,0.1), # Taux d'apprentissage
gamma = 0, # Valeur de réduction minimale du gain
colsample_bytree =  1, # Pourcentage de variables à utiliser dans chaque arbre
min_child_weight =1,
subsample=c(0.8,0.9)# Poids minimal des feuilles enfants
)
# Entraîner le modèle avec la grille de recherche
xgb_train <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = trainControl(method = "cv", number = 5),
tuneGrid = xgb_grid
)
# Afficher les meilleurs paramètres
print(xgb_train$bestTune)
xgb_final <- xgboost(
data = X_train,
label = y_train,
nrounds = xgb_train$bestTune$nrounds,
max_depth = xgb_train$bestTune$max_depth,
eta = xgb_train$bestTune$eta,
gamma = xgb_train$bestTune$gamma,
colsample_bytree = xgb_train$bestTune$colsample_bytree,
min_child_weight = xgb_train$bestTune$min_child_weight,
subsample=xgb_train$bestTune$subsample,
verbose = 0
)
# Faire des prédictions sur l'ensemble de test
hat_y_XGBoost<-as.data.frame(predict(xgb_final,newdata=testX))
xgb_final <- xgboost(
data = X_train,
label = y_train,
nrounds = xgb_train$bestTune$nrounds,
max_depth = xgb_train$bestTune$max_depth,
eta = xgb_train$bestTune$eta,
gamma = xgb_train$bestTune$gamma,
colsample_bytree = xgb_train$bestTune$colsample_bytree,
min_child_weight = xgb_train$bestTune$min_child_weight,
subsample=xgb_train$bestTune$subsample,
verbose = 0
)
# Faire des prédictions sur l'ensemble de test
hat_y_XGBoost<-as.data.frame(predict(xgb_final,newdata=testX2))
# Installation des packages
rm(list=ls())
library(tidyverse)
library(lme4) # lmer(): To fit mixed-model
library(lmerTest) # lmer (): To fit mixed-model and diplay p-values
library(nlme) # To fit mixed-model
library(lattice) # To plot mixed-model
library(nlme)
library(plotly)
library(ggplot2)
library(Metrics)
library(gridExtra)
library(dplyr)
library(MASS)
library(Matrix)
library(tidyverse)
library(xgboost)
library(caret)
library(readxl)
library(kableExtra)
library(knitr)
library(corrplot)
theme_set(theme_bw())
set.seed(2023)
train<-read.csv('train_maladie.csv', header = T, sep = ",",dec=".")
testX<-read.csv('test_X_maladie.csv', header = T, sep = ",",dec=".")
train%>% rmarkdown::paged_table()
train2<-train
# Suppression des colonnes qu'on ne veut pas normaliser
genre=train$genre
sujet=train$sujet
score=train$score
duree=train$duree
train=subset(train,select=-c(genre,sujet,score,duree))
# Normalisation des données
train=scale(train)
train=as.data.frame(train)
# Rajout des variables enlevées pour la normalisation
train$genre=genre
train$sujet=sujet
train$score=score
train$duree=duree
# Déclaration des variables sujet et genre comme des facteurs
train$sujet=as.factor(train$sujet)
train$genre=as.factor(train$genre)
# Entrainement de notre modèle final sur tout le jeu de données :
# Création de notre variable inflx :
train$genre=as.numeric(train$genre)
moment_inflexion=numeric(42)
# Aggrégation du jeu de données pour les durées similaires par sujet, pour éviter les problèmes calculatoires du taux de variation.
for (i in 1:42){
patient<-train %>% filter(sujet ==i)
patient$duree=round(patient$duree)
patient <- aggregate(patient[c("age","genre","score","FF","FF.Abs","FF.RAP","FF.PPQ5","FF.DDP","AV","AV.dB","AV.APQ3","AV.APQ5","AV.APQ11","AV.DDA","BTC1","BTC2","CDNL","EFS","VFNL")], by = list(patient$duree), FUN = mean)
names(patient)[names(patient) == "Group.1"] <- "duree"
coeff_directeur <- numeric(0)
# Calcul des taux de variations entre chaque observation pour tous les sujets
for (j in 1:(length(patient$duree)-1)) {
coeff_directeur <- c(coeff_directeur, (patient$score[j+1] - patient$score[j]) / (patient$duree[j+1] - patient$duree[j]))
}
# Calcul de l'écart entre les taux de variations en valeur absolue
variations=abs(diff(coeff_directeur))
#Recherche de l'écart maximal
indice_max_variation_patient <- which.max(variations)+1
#Stockage dans le vecteur moment_inflexion
moment_inflexion[i]=(patient$duree[indice_max_variation_patient]+patient$duree[indice_max_variation_patient+1])/2
}
train <- train %>%
group_by(sujet) %>%
mutate(inflexion = moment_inflexion[sujet]) %>%
ungroup()
# Création de la variable inflx :
train$inflx=(train$duree>train$inflexion)
# Retrait de la variable inflexion, inutile pour nous
train=subset(train,select=-inflexion)
# Transformation de la variable inflx en numérique
train$inflx=as.numeric(train$inflx)
mod_Final=lmer(score ~ duree+FF+AV+BTC1+CDNL+EFS+VFNL +(duree+BTC1+FF+AV+EFS+VFNL|  sujet:inflx), data = train, REML = FALSE)
RMSE_mod_Final= rmse(train$score,predict(mod_Final))
resultats_mod_Final=data.frame(
sujet=train$sujet,
duree=train$duree,
score=train$score,
predict=predict(mod_Final,train)
)
resultats_mod_Final %>% filter(sujet %in% selected) %>%
filter(sujet %in% selected) %>%
ggplot() +
geom_point(aes(x = duree, y = score, color = "Valeur du score"), size = 1) +
geom_line(aes(x = duree, y = predict, color = "Prédiction")) +
facet_wrap(~ sujet, ncol = 4) +
scale_color_manual(values = c("Valeur du score" = "red", "Prédiction" = "black"),
labels = c("Prédiction","Valeur du score")) +
labs(title = "Prédiction du mod_Final sur tout le jeu de données Train", color = "Légende")
selected=c(3,7,11,17,21,31,32,41)
train %>% filter(sujet %in% selected) %>%
ggplot() + geom_point(aes(x = FF.Abs, y = score)) + facet_wrap(~ sujet, ncol=2)
resultats_mod_Final=data.frame(
sujet=train$sujet,
duree=train$duree,
score=train$score,
predict=predict(mod_Final,train)
)
resultats_mod_Final %>% filter(sujet %in% selected) %>%
filter(sujet %in% selected) %>%
ggplot() +
geom_point(aes(x = duree, y = score, color = "Valeur du score"), size = 1) +
geom_line(aes(x = duree, y = predict, color = "Prédiction")) +
facet_wrap(~ sujet, ncol = 4) +
scale_color_manual(values = c("Valeur du score" = "red", "Prédiction" = "black"),
labels = c("Prédiction","Valeur du score")) +
labs(title = "Prédiction du mod_Final sur tout le jeu de données Train", color = "Légende")
testX2<-testX
# Suppression des colonnes qu'on ne veut pas normaliser
genre=testX$genre
sujet=testX$sujet
duree=testX$duree
testX=subset(testX,select=-c(genre,sujet,duree))
# Normalisation des données
testX=scale(testX)
testX=as.data.frame(testX)
# Rajout des variables enlevées pour la normalisation
testX$genre=genre
testX$sujet=sujet
testX$duree=duree
# Déclaration des variables sujet et genre comme des facteurs
testX$sujet=as.factor(testX$sujet)
testX$genre=as.factor(testX$genre)
# Ajout de la variable inflx
testX <- testX %>%
group_by(sujet) %>%
mutate(inflexion = moment_inflexion[sujet]) %>%
ungroup()
testX$inflx=(testX$duree>testX$inflexion)
testX=subset(testX,select=-inflexion)
testX$inflx=as.numeric(testX$inflx)
hat_y<-as.data.frame(predict(mod_Final,newdata=testX))
write_csv(hat_y,'hat_y.csv')
y_train <- train2$score
X_train <- as.matrix(train2[, -which(names(train2) == "score")])
X_test <- as.matrix(testX2)
# Entraîner le modèle XGBoost avec une grille de recherche des hyperparamètres
xgb_grid <- expand.grid(
nrounds = 150, # Nombre d'itérations
max_depth = c(10,11,12), # Profondeur maximale de l'arbre
eta = c(0.08,0.1), # Taux d'apprentissage
gamma = 0, # Valeur de réduction minimale du gain
colsample_bytree =  1, # Pourcentage de variables à utiliser dans chaque arbre
min_child_weight =1,
subsample=c(0.8,0.9)# Poids minimal des feuilles enfants
)
# Entraîner le modèle avec la grille de recherche
xgb_train <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = trainControl(method = "cv", number = 5),
tuneGrid = xgb_grid
)
# Afficher les meilleurs paramètres
print(xgb_train$bestTune)
xgb_final <- xgboost(
data = X_train,
label = y_train,
nrounds = xgb_train$bestTune$nrounds,
max_depth = xgb_train$bestTune$max_depth,
eta = xgb_train$bestTune$eta,
gamma = xgb_train$bestTune$gamma,
colsample_bytree = xgb_train$bestTune$colsample_bytree,
min_child_weight = xgb_train$bestTune$min_child_weight,
subsample=xgb_train$bestTune$subsample,
verbose = 0
)
# Faire des prédictions sur l'ensemble de test
hat_y_XGBoost<-as.data.frame(predict(xgb_final,newdata=testX2))
xgb_final <- xgboost(
data = X_train,
label = y_train,
nrounds = xgb_train$bestTune$nrounds,
max_depth = xgb_train$bestTune$max_depth,
eta = xgb_train$bestTune$eta,
gamma = xgb_train$bestTune$gamma,
colsample_bytree = xgb_train$bestTune$colsample_bytree,
min_child_weight = xgb_train$bestTune$min_child_weight,
subsample=xgb_train$bestTune$subsample,
verbose = 0
)
# Faire des prédictions sur l'ensemble de test
hat_y_XGBoost<-as.data.frame(predict(xgb_final,newdata=X_test))
write_csv(hat_y_XGBoost,'hat_y_XGBoost.csv')
View(X_test)
View(testX)
internet <- read.csv('newtest.csv', header = T, sep = ",",dec=".")
View(internet)
internet <- read.csv('newtest.csv', header = T, sep = ",",dec=".")
internet <- internet %>% filter(internet, internet$duree>0)
internet <- read.csv('newtest.csv', header = T, sep = ",",dec=".")
internet <- internet %>% filter(internet, duree>0)
internet <- read.csv('newtest.csv', header = T, sep = ",",dec=".")
internet <- internet %>% filter(duree > 0)
RMSE_false=rmse(internet$score,hat_y)
RMSE_false2=rmse(internet$score,hat_y_XGBoost)
print(RMSE_false)
print(RMSE_false2)
RMSE_false=sqrt(mean(hat_y-internet$score)^2)
RMSE_false2=sqrt(mean(hat_y_XGBoost-internet$score)^2)
print(RMSE_false)
print(RMSE_false2)
View(hat_y)
RMSE_false=rmse(hat_y$`predict(mod_Final, newdata = testX)`,internet$score)
RMSE_false2=rmse(hat_y_XGBoost$`predict(xgb_final, newdata = X_test)`,internet$score)
print(RMSE_false)
print(RMSE_false2)
mod_Final=lmer(score ~ duree+FF+AV+BTC1+CDNL+EFS+VFNL +(duree+BTC1+FF+AV+EFS+VFNL|  sujet:inflx), data = train, REML = FALSE)
xgb_final <- xgboost(
data = X_train,
label = y_train,
nrounds = xgb_train$bestTune$nrounds,
max_depth = xgb_train$bestTune$max_depth,
eta = xgb_train$bestTune$eta,
gamma = xgb_train$bestTune$gamma,
colsample_bytree = xgb_train$bestTune$colsample_bytree,
min_child_weight = xgb_train$bestTune$min_child_weight,
subsample=xgb_train$bestTune$subsample,
verbose = 0
)
RMSE_mod_XG_Boost= rmse(train$score,predict(xgb_final))
xgb_final <- xgboost(
data = X_train,
label = y_train,
nrounds = xgb_train$bestTune$nrounds,
max_depth = xgb_train$bestTune$max_depth,
eta = xgb_train$bestTune$eta,
gamma = xgb_train$bestTune$gamma,
colsample_bytree = xgb_train$bestTune$colsample_bytree,
min_child_weight = xgb_train$bestTune$min_child_weight,
subsample=xgb_train$bestTune$subsample,
verbose = 0
)
RMSE_mod_XG_Boost= rmse(train$score,predict(xgb_final,newdata = X_train))
RMSE_mod_XG_Boost %>%
kbl(caption = "RMSE du modèle XG_Boost sur tout le jeu de données Train", col.names = "RMSE") %>%
kable_styling()
resultats_mod5_valid=data.frame(
sujet=train$sujet,
duree=train$duree,
score=train$score,
predict=predict(mod_Final,train)
)
resultats_mod5_valid %>%
filter(sujet %in% selected) %>%
ggplot() +
geom_point(aes(x = duree, y = score, color = "Valeur du score"), size = 1) +
geom_line(aes(x = duree, y = predict, color = "Prédiction")) +
facet_wrap(~ sujet, ncol = 4) +
scale_color_manual(values = c("Valeur du score" = "red", "Prédiction" = "black"),
labels = c("Prédiction","Valeur du score")) +
labs(title = "Prédiction du modèle à effet mixte sur le jeu de données Validation",
color = "Légende")
resultats_mod5_valid=data.frame(
sujet=train$sujet,
duree=train$duree,
score=train$score,
predict=predict(mod_Final,train)
)
resultats_mod5_valid %>%
filter(sujet %in% selected) %>%
ggplot() +
geom_point(aes(x = duree, y = score, color = "Valeur du score"), size = 1) +
geom_line(aes(x = duree, y = predict, color = "Prédiction")) +
facet_wrap(~ sujet, ncol = 4) +
scale_color_manual(values = c("Valeur du score" = "red", "Prédiction" = "black"),
labels = c("Prédiction","Valeur du score")) +
labs(title = "Prédiction du modèle à effet mixte sur tout le jeu de données",
color = "Légende")
resultats_mod5_valid=data.frame(
sujet=train$sujet,
duree=train$duree,
score=train$score,
predict=predict(mod_Final,train)
)
resultats_mod5_valid %>%
filter(sujet %in% selected) %>%
ggplot() +
geom_point(aes(x = duree, y = score, color = "Valeur du score"), size = 1) +
geom_line(aes(x = duree, y = predict, color = "Prédiction")) +
facet_wrap(~ sujet, ncol = 4) +
scale_color_manual(values = c("Valeur du score" = "red", "Prédiction" = "black"),
labels = c("Prédiction","Valeur du score")) +
labs(title = "Prédiction du modèle Finale sur tout le jeu de données",
color = "Légende")
resultats_mod5_valid=data.frame(
sujet=train$sujet,
duree=train$duree,
score=train$score,
predict=predict(mod_Final,train)
)
resultats_mod5_valid %>%
filter(sujet %in% selected) %>%
ggplot() +
geom_point(aes(x = duree, y = score, color = "Valeur du score"), size = 1) +
geom_line(aes(x = duree, y = predict, color = "Prédiction")) +
facet_wrap(~ sujet, ncol = 4) +
scale_color_manual(values = c("Valeur du score" = "red", "Prédiction" = "black"),
labels = c("Prédiction","Valeur du score")) +
labs(title = "Prédiction du modèle mod_Final sur tout le jeu de données",
color = "Légende")
